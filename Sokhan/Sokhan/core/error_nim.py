import unicodedata
import string
import time
from functools import lru_cache
import concurrent.futures
import numpy as np
import csv

# Ø§ØµÙ„Ø§Ø­ Ø¬Ø¯ÙˆÙ„ ØªØ±Ø¬Ù…Ù‡ Ú©Ù‡ Ù†ÛŒÙ…â€ŒÙØ§ØµÙ„Ù‡ Ø±Ùˆ Ù†Ú¯Ù‡ Ø¯Ø§Ø±Ù‡
COMBINED_TRANS = str.maketrans('', '', string.punctuation + string.whitespace.replace('\u200c', '') + '@#')
NUMBER_TRANS = str.maketrans('0123456789', 'Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹')

class TurboNormalizer:
    def __init__(self):
        self.combined_trans = COMBINED_TRANS
        self.number_trans = NUMBER_TRANS
        self.diacritics = set('\u064b\u064c\u064d\u064e\u064f\u0650\u0651\u0652')
        self.suffixes = {'Ù‡Ø§', 'Ø§ÛŒ', 'ØªØ±', 'ØªØ±ÛŒ', 'ØªØ±ÛŒÙ†'}
    
    @lru_cache(maxsize=16384)
    def normalize_word(self, word: str) -> str:
        word = unicodedata.normalize('NFKC', word)
        word = ''.join(c for c in word if c not in self.diacritics)
        return word.translate(self.combined_trans).translate(self.number_trans).lower()
    
    def _normalize_chunk(self, sentence: str) -> str:
        sentence = unicodedata.normalize('NFKC', sentence.lower())
        sentence = ''.join(c for c in word if c not in self.diacritics)
        sentence = sentence.translate(self.combined_trans).translate(self.number_trans)
        
        # Ú©Ø§Ù‡Ø´ ØªÚ©Ø±Ø§Ø± Ø­Ø±ÙˆÙ
        result = []
        last_char = ''
        count = 0
        for char in sentence:
            if char == last_char:
                count += 1
                if count <= 2:
                    result.append(char)
            else:
                result.append(char)
                last_char = char
                count = 1
        sentence = ''.join(result)
        
        # Ø¬Ø¯Ø§ Ú©Ø±Ø¯Ù† Ú©Ù„Ù…Ø§Øª Ø¨Ø§ Ø­ÙØ¸ Ù†ÛŒÙ…â€ŒÙØ§ØµÙ„Ù‡
        words = sentence.split()
        output = []
        i = 0
        while i < len(words):
            word = words[i]
            # Ù…Ø¯ÛŒØ±ÛŒØª "Ù…ÛŒ" Ùˆ "Ù†Ù…ÛŒ"
            if word.startswith('Ù…ÛŒ') or word.startswith('Ù†Ù…ÛŒ'):
                prefix = 'Ù…ÛŒ' if word.startswith('Ù…ÛŒ') else 'Ù†Ù…ÛŒ'
                rest = word[len(prefix):]
                if rest:
                    word = prefix + '\u200c' + rest
            
            # Ù†ÛŒÙ…â€ŒÙØ§ØµÙ„Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ù¾Ø³ÙˆÙ†Ø¯Ù‡Ø§
            if i + 1 < len(words) and words[i + 1] in self.suffixes:
                output.append(word + '\u200c' + words[i + 1])
                i += 2
            else:
                output.append(word)
                i += 1
        
        return ' '.join(output)
    
    def normalize_sentence(self, sentence: str) -> str:
        return self._normalize_chunk(sentence)
    
    def normalize_bulk(self, texts: list) -> list:
        if len(texts) < 100:
            return [self._normalize_chunk(t) for t in texts]
        text_array = np.array(texts, dtype=object)
        with concurrent.futures.ThreadPoolExecutor() as executor:
            results = list(executor.map(self._normalize_chunk, text_array))
        return results

def precision_timer(func):
    def wrapper(*args, **kwargs):
        func(*args, **kwargs)
        start = time.perf_counter_ns()
        result = func(*args, **kwargs)
        elapsed = (time.perf_counter_ns() - start) / 1e9
        return result, elapsed
    return wrapper

# Ø¯ÛŒØªØ§Ø³Øª Ù†Ù…ÙˆÙ†Ù‡ ØªÙˆÛŒÛŒØªØ±
twitter_dataset = [
    "Ø³Ù„Ø§Ù… Ú†Ø·ÙˆØ±ÛŒØŸ Ø§Ù…Ø±ÙˆØ² Ø®ÛŒÙ„ÛŒ Ù‡ÙˆØ§ Ø®ÙˆØ¨Ù‡!",
    "Ø§ÛŒÙ† ÙÛŒÙ„Ù… Ø¬Ø¯ÛŒØ¯ Ø±Ùˆ Ø¯ÛŒØ¯Ù… ÙˆØ§Ù‚Ø¹Ø§ Ø¹Ø§Ù„ÛŒ Ø¨ÙˆØ¯!!!",
    "Ú†Ø±Ø§ Ø§Ù†Ù‚Ø¯Ø± ØªØ±Ø§ÙÛŒÚ© Ø²ÛŒØ§Ø¯Ù‡ØŸØŸØŸ Ø®Ø³ØªÙ‡ Ø´Ø¯Ù…",
    "Ú©Ø§Ø´ ÛŒÙ‡ Ø±ÙˆØ² Ø¨Ø¯ÙˆÙ† Ø§Ø³ØªØ±Ø³ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒÙ…...",
    "Ø¯ÛŒØ±ÙˆØ² ÛŒÙ‡ Ú©ØªØ§Ø¨ Ø®ÙˆÙ†Ø¯Ù… Ø®ÛŒÙ„ÛŒ Ø¬Ø§Ù„Ø¨ Ø¨ÙˆØ¯ #Ú©ØªØ§Ø¨",
    # ... Ø¨Ù‚ÛŒÙ‡ Ø¯ÛŒØªØ§Ø³Øª Ù‡Ù…ÙˆÙ†ÛŒÙ‡ Ú©Ù‡ Ù‚Ø¨Ù„Ø§Ù‹ Ø¯Ø§Ø¯Ù…ØŒ Ø¨Ø±Ø§ÛŒ Ú©ÙˆØªØ§Ù‡ Ø¨ÙˆØ¯Ù† ÙÙ‚Ø· Ûµ ØªØ§ Ù†Ù…ÙˆÙ†Ù‡ Ú¯Ø°Ø§Ø´ØªÙ…
    # Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒ Ø¯ÛŒØªØ§Ø³Øª Ú©Ø§Ù…Ù„ Ø±Ùˆ Ø§Ø² Ù¾ÛŒØ§Ù… Ù‚Ø¨Ù„ÛŒ Ú©Ù¾ÛŒ Ú©Ù†ÛŒ
]

if __name__ == "__main__":
    normalizer = TurboNormalizer()
    
    # Ø¯ÛŒØªØ§Ø³Øª Ú©Ø§Ù…Ù„ Ø±Ùˆ Ø§ÛŒÙ†Ø¬Ø§ Ú©Ù¾ÛŒ Ú©Ù† ÛŒØ§ Ù‡Ù…ÙˆÙ† Ûµ ØªØ§ Ø±Ùˆ ØªØ³Øª Ú©Ù†
    twitter_dataset = twitter_dataset  # Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø¨Ø§ Ø¯ÛŒØªØ§Ø³Øª Ú©Ø§Ù…Ù„ Ø§Ú¯Ù‡ Ø®ÙˆØ§Ø³ØªÛŒ
    
    tests = {
        "ØªÚ© Ú©Ù„Ù…Ù‡ Ø³Ø§Ø¯Ù‡": ("TEST!", normalizer.normalize_word),
        "Ø¬Ù…Ù„Ù‡ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯": ("Ø§ÛŒÙ† ÛŒÚ© Ù…ØªÙ† TEST Ø¨Ø§ Ø¹Ù„Ø§Ø¦Ù… !@# Ø³Ø¬Ø§ÙˆÙ†Ø¯ÛŒ Ùˆ   ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ø§Ø³Øª!!!", normalizer.normalize_sentence),
        "Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú¯Ø±ÙˆÙ‡ÛŒ": (twitter_dataset, normalizer.normalize_bulk),
        "ØªÙˆÛŒÛŒØª Ù†Ù…ÙˆÙ†Ù‡": ("Ø³Ù„Ø§Ù… @ali Ú†Ø·ÙˆØ±ÛŒØŸ #Ø¬Ø§Ù„Ø¨ Ø§ÛŒÙ†Ùˆ Ø¨Ø¨ÛŒÙ†Ù…Ù…Ù… https://t.co/test ğŸ˜‚ Û³.Û±Û´", normalizer.normalize_sentence)
    }
    
    with open('normalized_tweets.csv', 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(['Original Text', 'Normalized Text', 'Execution Time (s)'])
        
        for name, (data, func) in tests.items():
            result, elapsed = precision_timer(func)(data)
            print(f"âœ… ØªØ³Øª {name}")
            print(f"â± Ø²Ù…Ø§Ù† Ø§Ø¬Ø±Ø§: {elapsed:.10f} Ø«Ø§Ù†ÛŒÙ‡")
            print(f"ğŸ“Š Ø­Ø¬Ù… Ø¯Ø§Ø¯Ù‡: {len(data) if isinstance(data, list) else len(data.split())}")
            
            if isinstance(result, str):
                writer.writerow([data, result, elapsed])
                print(f"ğŸ¯ Ù†Ù…ÙˆÙ†Ù‡ Ø®Ø±ÙˆØ¬ÛŒ: {result[:75] + '...' if len(result) > 75 else result}")
            else:
                for orig, norm in zip(data, result):
                    writer.writerow([orig, norm, elapsed / len(data)])
                print(f"ğŸ¯ Ù†Ù…ÙˆÙ†Ù‡ Ø®Ø±ÙˆØ¬ÛŒ: {result[0][:75] + '...' if len(result[0]) > 75 else result[0]}")
            print("-" * 80)
    
    print("Ù†ØªØ§ÛŒØ¬ Ø¯Ø± ÙØ§ÛŒÙ„ 'normalized_tweets.csv' Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯!")