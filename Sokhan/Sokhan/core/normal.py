import unicodedata
import string
import time
from functools import lru_cache
import concurrent.futures
import numpy as np

# ุฌุฏุงูู ุงุฒ ูพุดโุณุงุฎุชู ุจุง ุฏุณุชุฑุณ ุณุฑุนโุชุฑ
PUNCTUATION_TRANS = str.maketrans('', '', string.punctuation)
WHITESPACE_TRANS = str.maketrans('', '', string.whitespace)

class TurboNormalizer:
    def __init__(self):
        # ุชุจุฏู ุฌุฏุงูู ุจู ูุฑู ูุงุจู ุงุณุชูุงุฏู ูุณุชูู
        self.combined_trans = str.maketrans('', '', string.punctuation + string.whitespace)
    
    @lru_cache(maxsize=32768)  # ฺฉุงูุด ุงูุฏุงุฒู ฺฉุด ุจุฑุง ุณุฑุนุช ุจุดุชุฑ ุฏุฑ ุฏุณุชุฑุณ
    def normalize_word(self, word: str) -> str:
        return unicodedata.normalize('NFKC', word).translate(self.combined_trans).lower()
    
    def _normalize_chunk(self, sentence: str) -> str:
        """ูพุฑุฏุงุฒุด ุณุฑุน ฺฉ ุชฺฉู ูุชู"""
        return ' '.join(
            unicodedata.normalize('NFKC', sentence.lower())
            .translate(self.combined_trans)
            .split()
        )
    
    def normalize_sentence(self, sentence: str) -> str:
        return self._normalize_chunk(sentence)
    
    def normalize_bulk(self, texts: list) -> list:
        # ุงุณุชูุงุฏู ุงุฒ ูพุฑุฏุงุฒุด ููุงุฒ ุจุฑุง ุฏุงุฏูโูุง ุจุฒุฑฺฏ
        if len(texts) < 100:  # ุจุฑุง ุฏุงุฏูโูุง ฺฉูฺฺฉ ุงุฒ ุฑูุด ุณุงุฏู ุงุณุชูุงุฏู ฺฉู
            return [self._normalize_chunk(t) for t in texts]
        
        # ุชุจุฏู ุจู ุขุฑุงู NumPy ุจุฑุง ูุฏุฑุช ุจูุชุฑ ุญุงูุธู
        text_array = np.array(texts, dtype=str)
        chunk_size = max(1, len(texts) // (4 * concurrent.futures.ThreadPoolExecutor()._max_workers))
        
        # ูพุฑุฏุงุฒุด ููุงุฒ
        with concurrent.futures.ThreadPoolExecutor() as executor:
            results = list(executor.map(
                self._normalize_chunk,
                text_array
            ))
        return results

def precision_timer(func):
    def wrapper(*args, **kwargs):
        # ฺฏุฑู ฺฉุฑุฏู ุจุฏูู ุญููู ุงุถุงู
        func(*args, **kwargs)
        
        start = time.perf_counter_ns()
        result = func(*args, **kwargs)
        elapsed = (time.perf_counter_ns() - start) / 1e9
        return result, elapsed
    return wrapper

if __name__ == "__main__":
    normalizer = TurboNormalizer()
    sample_text = "ุงู ฺฉ ูุชู TEST ุจุง ุนูุงุฆู !@# ุณุฌุงููุฏ ู   ูุงุตููโูุง ุงุถุงูู ุงุณุช!"
    large_dataset = [sample_text * 100 for _ in range(1000)]
    
    tests = {
        "ุชฺฉ ฺฉููู ุณุงุฏู": ("TEST!", normalizer.normalize_word),
        "ุฌููู ุงุณุชุงูุฏุงุฑุฏ": (sample_text, normalizer.normalize_sentence),
        "ูพุฑุฏุงุฒุด ฺฏุฑูู": (large_dataset, normalizer.normalize_bulk)
    }
    
    for name, (data, func) in tests.items():
        result, elapsed = precision_timer(func)(data)
        print(f"โ ุชุณุช {name}")
        print(f"โฑ ุฒูุงู ุงุฌุฑุง: {elapsed:.10f} ุซุงูู")
        print(f"๐ ุญุฌู ุฏุงุฏู: {len(data) if isinstance(data, list) else len(data.split())}")
        print(f"๐ฏ ููููู ุฎุฑูุฌ: {result[:75] + '...' if isinstance(result, str) else result[0][:75] + '...'}")
        print("-" * 80)