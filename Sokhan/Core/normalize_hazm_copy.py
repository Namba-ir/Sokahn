import unicodedata
import string
import time
from functools import lru_cache
import concurrent.futures
import numpy as np
import re

# Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ø² Ù¾ÛŒØ´â€ŒØ³Ø§Ø®ØªÙ‡
COMBINED_TRANS = str.maketrans('', '', string.punctuation + string.whitespace + '@#')
NUMBER_TRANS = str.maketrans('0123456789', 'Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹')

class TurboNormalizer:
    def __init__(self):
        self.combined_trans = COMBINED_TRANS
        self.number_trans = NUMBER_TRANS
        # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø§Ø¹Ø±Ø§Ø¨ Ùˆ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø®Ø§Øµ
        self.diacritics_pattern = r'[\u064b-\u0652]'  # FATHATAN ØªØ§ SUKUN
        self.special_chars_pattern = r'[\u0600-\u06FF\ufb50-\ufdff\ufe70-\ufeff]'
        self.repeat_pattern = r'(.)\1{2,}'  # Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ ØªÚ©Ø±Ø§Ø± Ø­Ø±ÙˆÙ
        
    @lru_cache(maxsize=32768)
    def normalize_word(self, word: str) -> str:
        word = unicodedata.normalize('NFKC', word)
        word = re.sub(self.diacritics_pattern, '', word)  # Ø­Ø°Ù Ø§Ø¹Ø±Ø§Ø¨
        word = word.translate(self.number_trans)  # ØªØ¨Ø¯ÛŒÙ„ Ø§Ø¹Ø¯Ø§Ø¯
        return word.translate(self.combined_trans).lower()
    
    def _normalize_chunk(self, sentence: str) -> str:
        sentence = unicodedata.normalize('NFKC', sentence.lower())
        sentence = re.sub(self.diacritics_pattern, '', sentence)  # Ø­Ø°Ù Ø§Ø¹Ø±Ø§Ø¨
        sentence = sentence.translate(self.number_trans)  # ØªØ¨Ø¯ÛŒÙ„ Ø§Ø¹Ø¯Ø§Ø¯
        
        # Ú©Ø§Ù‡Ø´ ØªÚ©Ø±Ø§Ø± Ø­Ø±ÙˆÙ (Ø¨ÛŒØ´ Ø§Ø² Û² Ø¨Ù‡ Û²)
        sentence = re.sub(self.repeat_pattern, r'\1\1', sentence)
        
        # Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ø³Ø¨Ú© ÙØ§Ø±Ø³ÛŒ
        sentence = sentence.replace('"', 'Â«').replace('"', 'Â»').replace('...', 'â€¦').replace('.', 'Ù«')
        
        # Ø¬Ø¯Ø§ Ú©Ø±Ø¯Ù† "Ù…ÛŒ" Ùˆ "Ù†Ù…ÛŒ" Ø¨Ø§ Ù†ÛŒÙ…â€ŒÙØ§ØµÙ„Ù‡
        sentence = re.sub(r'\b(Ù†?Ù…ÛŒ)([^ ]+)', r'\1â€Œ\2', sentence)
        
        # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø®Ø§Øµ Ùˆ ÙØ§ØµÙ„Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ
        sentence = re.sub(self.special_chars_pattern, '', sentence)
        words = sentence.split()
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù†ÛŒÙ…â€ŒÙØ§ØµÙ„Ù‡ Ø¨Ø±Ø§ÛŒ Ù¾Ø³ÙˆÙ†Ø¯Ù‡Ø§ÛŒ Ø±Ø§ÛŒØ¬
        for i in range(len(words)-1, 0, -1):
            if words[i] in {'Ù‡Ø§', 'Ø§ÛŒ', 'ØªØ±', 'ØªØ±ÛŒ', 'ØªØ±ÛŒÙ†'}:
                words[i-1] = words[i-1] + 'â€Œ' + words[i]
                words.pop(i)
        
        return ' '.join(words)
    
    def normalize_sentence(self, sentence: str) -> str:
        return self._normalize_chunk(sentence)
    
    def normalize_bulk(self, texts: list) -> list:
        if len(texts) < 100:
            return [self._normalize_chunk(t) for t in texts]
        
        text_array = np.array(texts, dtype=str)
        chunk_size = max(1, len(texts) // (4 * concurrent.futures.ThreadPoolExecutor()._max_workers))
        
        with concurrent.futures.ThreadPoolExecutor() as executor:
            results = list(executor.map(self._normalize_chunk, text_array))
        return results

def precision_timer(func):
    def wrapper(*args, **kwargs):
        func(*args, **kwargs)
        start = time.perf_counter_ns()
        result = func(*args, **kwargs)
        elapsed = (time.perf_counter_ns() - start) / 1e9
        return result, elapsed
    return wrapper

if __name__ == "__main__":
    normalizer = TurboNormalizer()
    sample_text = "Ø§ÛŒÙ† ÛŒÚ© Ù…ØªÙ† TEST Ø¨Ø§ Ø¹Ù„Ø§Ø¦Ù… !@# Ø³Ø¬Ø§ÙˆÙ†Ø¯ÛŒ Ùˆ   ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ø§Ø³Øª!!!"
    large_dataset = [sample_text * 100 for _ in range(1000)]
    twitter_sample = "Ø³Ù„Ø§Ù… @ali Ú†Ø·ÙˆØ±ÛŒØŸ #Ø¬Ø§Ù„Ø¨ Ø§ÛŒÙ†Ùˆ Ø¨Ø¨ÛŒÙ†Ù…Ù…Ù… https://t.co/test ðŸ˜‚ Û³.Û±Û´"
    
    tests = {
        "ØªÚ© Ú©Ù„Ù…Ù‡ Ø³Ø§Ø¯Ù‡": ("TEST!", normalizer.normalize_word),
        "Ø¬Ù…Ù„Ù‡ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯": (sample_text, normalizer.normalize_sentence),
        "Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú¯Ø±ÙˆÙ‡ÛŒ": (large_dataset, normalizer.normalize_bulk),
        "ØªÙˆÛŒÛŒØª Ù†Ù…ÙˆÙ†Ù‡": (twitter_sample, normalizer.normalize_sentence)
    }
    
    for name, (data, func) in tests.items():
        result, elapsed = precision_timer(func)(data)
        print(f"âœ… ØªØ³Øª {name}")
        print(f"â± Ø²Ù…Ø§Ù† Ø§Ø¬Ø±Ø§: {elapsed:.10f} Ø«Ø§Ù†ÛŒÙ‡")
        print(f"ðŸ“Š Ø­Ø¬Ù… Ø¯Ø§Ø¯Ù‡: {len(data) if isinstance(data, list) else len(data.split())}")
        print(f"ðŸŽ¯ Ù†Ù…ÙˆÙ†Ù‡ Ø®Ø±ÙˆØ¬ÛŒ: {result[:75] + '...' if isinstance(result, str) else result[0][:75] + '...'}")
        print("-" * 80)